- title: "ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds"
  authors: Daniel Seita, Yufei Wang, Edward Yao Li, Sarthak J Shetty, Zackory Erickson, and David Held
  year: 2022
  type: conference
  venue: CoRL
  image: ../images/toolflownet.jpg
  id: seita2022toolflownet
  projectpage: https://sites.google.com/view/point-cloud-policy/home
  code: 
  bibtex: |
      @inproceedings{seita2022toolflownet,
          title={ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds},
          author={Daniel Seita and Yufei Wang and Edward Yao Li and Sarthak J Shetty and Zackory Erickson and David Held},
          booktitle={6th Annual Conference on Robot Learning},
          year={2022},
          url={https://openreview.net/forum?id=2gfB_kMVFvP}
      }
  abstract: "Point clouds are a widely available and canonical data modality which conveys the 3D geometry of a scene. Despite significant progress in classification and segmentation from point clouds, policy learning from such a modality remains challenging, and most prior works in imitation learning focus on learning policies from images or state information. In this paper, we propose a novel framework for learning policies from point clouds for robotic manipulation with tools. We use a novel neural network, ToolFlowNet, which predicts dense per-point flow on the tool that the robot controls, and then uses the flow to derive the transformation that the robot should execute. We apply this framework to imitation learning of challenging deformable object manipulation tasks with continuous movement of tools, including scooping and pouring, and demonstrate significantly improved performance over baselines which do not use flow. We perform physical scooping experiments with ToolFlowNet and find that we can attain 78% scooping success."
  awards: 
  video: 
  pdf: https://openreview.net/pdf?id=2gfB_kMVFvP

- title: "Learning Representations that Enable Generalization in Assistive Tasks"
  authors: Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Aditi Raghunathan, and Anca Dragan
  year: 2022
  type: conference
  venue: CoRL
  image: ../images/he2022learning.jpg
  id: he2022learning
  projectpage: 
  code: 
  bibtex: |
      @inproceedings{he2022learning,
          title={Learning Representations that Enable Generalization in Assistive Tasks},
          author={Jerry Zhi-Yang He and Zackory Erickson and Daniel S. Brown and Aditi Raghunathan and Anca Dragan},
          booktitle={6th Annual Conference on Robot Learning},
          year={2022},
          url={https://openreview.net/forum?id=b88HF4vd_ej}
      }
  abstract: "Recent work in sim2real has successfully enabled robots to act in physical environments by training in simulation with a diverse ``population'' of environments (i.e. domain randomization). In this work, we focus on enabling generalization in assistive tasks: tasks in which the robot is acting to assist a user (e.g. helping someone with motor impairments with bathing or with scratching an itch). Such tasks are particularly interesting relative to prior sim2real successes because the environment now contains a human who is also acting. This complicates the problem because the diversity of human users (instead of merely physical environment parameters) is more difficult to capture in a population, thus increasing the likelihood of encountering out-of-distribution (OOD) human policies at test time. We advocate that generalization to such OOD policies benefits from (1) learning a good latent representation for human policies that test-time humans can accurately be mapped to, and (2) making that representation adaptable with test-time interaction data, instead of relying on it to perfectly capture the space of human policies based on the simulated population only. We study how to best learn such a representation by evaluating on purposefully constructed OOD test policies. We find that sim2real methods that encode environment (or population) parameters and work well in tasks that robots do in isolation, do not work well in assistance.  In assistance, it seems crucial to train the representation based on the history of interaction directly, because that is what the robot will have access to at test time. Further, training these representations to then predict human actions not only gives them better structure, but also enables them to be fine-tuned at test-time, when the robot observes the partner act."
  awards: 
  video: 
  pdf: https://openreview.net/pdf?id=b88HF4vd_ej

- title: "Visual Haptic Reasoning: Estimating Contact Forces by Observing Deformable Object Interactions"
  authors: Yufei Wang, David Held, and Zackory Erickson
  year: 2022
  type: journal
  venue: IEEE Robotics and Automation Letters (RA-L)
  image: ../images/vhr.jpg
  id: wang2022visual
  projectpage: https://sites.google.com/view/visualhapticreasoning/home
  code: 
  bibtex: |
      @article{wang2022visual,
          title={Visual Haptic Reasoning: Estimating Contact Forces by Observing Deformable Object Interactions},
          author={Wang, Yufei and Held, David and Erickson, Zackory},
          journal={IEEE Robotics and Automation Letters},
          year={2022},
          organization={IEEE}
      }
  abstract: "Robotic manipulation of highly deformable cloth presents a promising opportunity to assist people with several daily tasks, such as washing dishes; folding laundry; or dressing, bathing, and hygiene assistance for individuals with severe motor impairments. In this work, we introduce a formulation that enables a collaborative robot to perform visual haptic reasoning with cloth---the act of inferring the location and magnitude of applied forces during physical interaction. We present two distinct model representations, trained in physics simulation, that enable haptic reasoning using only visual and robot kinematic observations. We conducted quantitative evaluations of these models in simulation for robot-assisted dressing, bathing, and dish washing tasks, and demonstrate that the trained models can generalize across different tasks with varying interactions, human body sizes, and object shapes. We also present results with a real-world mobile manipulator, which used our simulation-trained models to estimate applied contact forces while performing physically assistive tasks with cloth. Videos can be found at our project webpage."
  awards: 
  video: 
  pdf: https://arxiv.org/pdf/2208.05632.pdf

- title: "CapSense: A Real-Time Capacitive Sensor Simulation Framework for Physical Human-Robot Interaction"
  authors: Christian Sch√∂ffmann, Zackory Erickson, and Hubert Zangl
  year: 2022
  type: journal
  venue: IEEE Robotics and Automation Letters (RA-L)
  image: ../images/capsense.jpg
  id: schoffmann2022capsense
  projectpage: 
  code: https://github.com/chstetco/capsense
  bibtex: |
      @article{schoffmann2022capsense,
          author={Sch{\"o}ffmann, Christian and Erickson, Zackory and Zangl, Hubert},
          journal={IEEE Robotics and Automation Letters}, 
          title={CapSense: A Real-Time Capacitive Sensor Simulation Framework for Physical Human-Robot Interaction}, 
          year={2022}
      }
  abstract: "This article presents CapSense, a real-time open-source capacitive sensor simulation framework for robotic applications. CapSense provides raw data of capacitive proximity sensors based on a fast and efficient 3D finite-element method (FEM) implementation. The proposed framework is interfaced to off-the-shelf robot and physics simulation environments to couple dynamic interaction of the environment with an electro-static solver for capacitance computation in real-time. The FEM method proposed in this article relies on a static tetrahedral mesh of the sensor surrounding without a-posteriori re-meshing and achieves high update rates by an adaptive update step. CapSense is flexible due to various configuration parameters (i.e. number, size, shape and location of electrodes) and serves as a platform for investigation of capacitive sensors in robotic applications. By using the proposed framework, researchers can simulate capacitive sensors in different scenarios and investigate these sensors and their configuration prior to installation and fabrication of real hardware. The proposed framework opens new research opportunities via sim-to-real transfer of capacitive sensing. The simulation approach is validated by comparing real-world results of different scenarios with simulation results. In order to showcase the benefits of CapSense in physical Human-Robot Interaction (pHRI), the framework is evaluated in a robotic healthcare scenario."
  awards: 
  video: 
  pdf: https://ieeexplore.ieee.org/document/9833265

- title: "Characterizing Multidimensional Capacitive Servoing for Physical Human-Robot Interaction"
  authors: Zackory Erickson, Henry M. Clever, Vamsee Gangaram, Eliot Xing, Greg Turk, C. Karen Liu, and Charles C. Kemp
  year: 2022
  type: journal
  venue: IEEE Transactions on Robotics (T-RO)
  image: ../images/capservoing.jpg
  id: erickson2022characterizing
  projectpage: 
  code: 
  bibtex: |
      @article{erickson2022characterizing,
          title={Characterizing Multidimensional Capacitive Servoing for Physical Human-Robot Interaction},
          author={Erickson, Zackory and Clever, Henry M and Gangaram, Vamsee and Xing, Eliot and Turk, Greg and Liu, C Karen and Kemp, Charles C},
          journal={IEEE Transactions on Robotics (T-RO)},
          year={2022}
      }
  abstract: "Towards the goal of robots performing robust and intelligent physical interactions with people, it is crucial that robots are able to accurately sense the human body, follow trajectories around the body, and track human motion. This study introduces a capacitive servoing control scheme that allows a robot to sense and navigate around human limbs during close physical interactions. Capacitive servoing leverages temporal measurements from a multi-electrode capacitive sensor array mounted on a robot's end effector to estimate the relative position and orientation (pose) of a nearby human limb. Capacitive servoing then uses these human pose estimates from a data-driven pose estimator within a feedback control loop in order to maneuver the robot's end effector around the surface of a human limb. We provide a design overview of capacitive sensors for human-robot interaction and then investigate the performance and generalization of capacitive servoing through an experiment with 12 human participants. The results indicate that multidimensional capacitive servoing enables a robot's end effector to move proximally or distally along human limbs while adapting to human pose. Using a cross-validation experiment, results further show that capacitive servoing generalizes well across people with different body size."
  awards: 
  video: 
  pdf: https://arxiv.org/pdf/2105.11582.pdf

- title: "Bodies Uncovered: Learning to Manipulate Real Blankets Around People via Physics Simulations"
  authors: Kavya Puthuveetil, Charles C. Kemp, and Zackory Erickson
  year: 2022
  type: journal
  venue: IEEE Robotics and Automation Letters (RA-L)
  image: ../images/bodies_uncovered.jpg
  id: puthuveetil2022bodies
  projectpage: https://rchi-lab.github.io/bodies-uncovered/
  code: https://github.com/RCHI-Lab/bodies-uncovered
  bibtex: |
      @article{puthuveetil2022bodies,
          author={Puthuveetil, Kavya and Kemp, Charles C. and Erickson, Zackory},
          journal={IEEE Robotics and Automation Letters}, 
          title={Bodies Uncovered: Learning to Manipulate Real Blankets Around People via Physics Simulations}, 
          year={2022},
          volume={7},
          number={2},
          pages={1984-1991},
          doi={10.1109/LRA.2022.3142732}
      }
  abstract: "While robots present an opportunity to provide physical assistance to older adults and people with mobility impairments in bed, people frequently rest in bed with blankets that cover the majority of their body. To provide assistance for many daily self-care tasks, such as bathing, dressing, or ambulating, a caregiver must first uncover blankets from part of a person's body. In this work, we introduce a formulation for robotic bedding manipulation around people in which a robot uncovers a blanket from a target body part while ensuring the rest of the human body remains covered. We compare two approaches for optimizing policies which provide a robot with grasp and release points that uncover a target part of the body: 1) reinforcement learning and 2) self-supervised learning with optimization to generate training data. We trained and conducted evaluations of these policies in physics simulation environments that consist of a deformable cloth mesh covering a simulated human lying supine on a bed. In addition, we transfer simulation-trained policies to a real mobile manipulator and demonstrate that it can uncover a blanket from target body parts of a manikin lying in bed. Source code is available online."
  awards: 
  video: https://www.youtube.com/watch?v=oTO2NqfSAXw
  pdf: https://arxiv.org/pdf/2109.04930.pdf


# - title: "Characterizing Multidimensional Capacitive Servoing for Physical Human-Robot Interaction"
#   authors: Zackory Erickson, Henry M. Clever, Vamsee Gangaram, Eliot Xing, Greg Turk, C. Karen Liu, Charles C. Kemp
#   year: 2021
#   type: preprint
#   venue: arXiv preprint
#   image: ../images/assistive_gym.jpg
#   id: erickson2021characterizing
#   projectpage: 
#   code: 
#   bibtex: |
#       @article{erickson2021characterizing,
#           title={Characterizing Multidimensional Capacitive Servoing for Physical Human-Robot Interaction},
#           author={Erickson, Zackory and Clever, Henry M and Gangaram, Vamsee and Xing, Eliot and Turk, Greg and Liu, C Karen and Kemp, Charles C},
#           journal={arXiv preprint arXiv:2105.11582},
#           year={2021}
#       }
#   abstract: "Towards the goal of robots performing robust and intelligent physical interactions with people, it is crucial that robots are able to accurately sense the human body, follow trajectories around the body, and track human motion. This study introduces a capacitive servoing control scheme that allows a robot to sense and navigate around human limbs during close physical interactions. Capacitive servoing leverages temporal measurements from a multi-electrode capacitive sensor array mounted on a robot's end effector to estimate the relative position and orientation (pose) of a nearby human limb. Capacitive servoing then uses these human pose estimates from a data-driven pose estimator within a feedback control loop in order to maneuver the robot's end effector around the surface of a human limb. We provide a design overview of capacitive sensors for human-robot interaction and then investigate the performance and generalization of capacitive servoing through an experiment with 12 human participants. The results indicate that multidimensional capacitive servoing enables a robot's end effector to move proximally or distally along human limbs while adapting to human pose. Using a cross-validation experiment, results further show that capacitive servoing generalizes well across people with different body size."
#   awards: 
#   video: 
#   pdf: https://arxiv.org/pdf/2105.11582.pdf

# - title: "Assistive Gym: A Physics Simulation Framework for Assistive Robotics"
#   authors: Zackory Erickson, Vamsee Gangaram, Ariel Kapusta, C. Karen Liu, and Charles C. Kemp
#   year: 2020
#   type: conference
#   venue: IEEE International Conference on Robotics and Automation (ICRA)
#   image: ../images/assistive_gym.jpg
#   id: erickson2020assistive
#   projectpage:
#   code: https://github.com/Healthcare-Robotics/assistive-gym
#   bibtex: |
#       @inproceedings{erickson2020assistive,
#           title={Assistive gym: A physics simulation framework for assistive robotics},
#           author={Erickson, Zackory and Gangaram, Vamsee and Kapusta, Ariel and Liu, C Karen and Kemp, Charles C},
#           booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
#           pages={10169--10176},
#           year={2020},
#           organization={IEEE}
#       }
#   abstract: "Autonomous robots have the potential to serve as versatile caregivers that improve quality of life for millions of people worldwide. Yet, conducting research in this area presents numerous challenges, including the risks of physical interaction between people and robots. Physics simulations have been used to optimize and train robots for physical assistance, but have typically focused on a single task. In this paper, we present Assistive Gym, an open source physics simulation framework for assistive robots that models multiple tasks. It includes six simulated environments in which a robotic manipulator can attempt to assist a person with activities of daily living (ADLs): itch scratching, drinking, feeding, body manipulation, dressing, and bathing. Assistive Gym models a person's physical capabilities and preferences for assistance, which are used to provide a reward function. We present baseline policies trained using reinforcement learning for four different commercial robots in the six environments. We demonstrate that modeling human motion results in better assistance and we compare the performance of different robots. Overall, we show that Assistive Gym is a promising tool for assistive robotics research."
#   awards: 
#   video: https://www.youtube.com/watch?v=EFKqNKO3P60
#   pdf: https://arxiv.org/pdf/1910.04700.pdf
